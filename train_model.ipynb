{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb273478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download required NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    logger.info(\"Downloading required NLTK resources...\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Timer decorator for performance monitoring\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"{func.__name__} took {end_time - start_time:.2f} seconds to execute\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Enhanced function to preprocess text\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to preprocess\n",
    "    remove_stopwords : bool\n",
    "        Whether to remove stopwords\n",
    "    lemmatize : bool\n",
    "        Whether to lemmatize words\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # Add custom domain-specific stopwords\n",
    "        custom_stopwords = {'job', 'work', 'company', 'position', 'required', 'requirements',\n",
    "                           'experience', 'skill', 'skills', 'candidate', 'opportunity', 'role'}\n",
    "        stop_words.update(custom_stopwords)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove short words (length < 3)\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "@timer_decorator\n",
    "def preprocess_dataframe(df, text_columns):\n",
    "    \"\"\"\n",
    "    Preprocess text columns in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to preprocess\n",
    "    text_columns : list\n",
    "        List of text columns to preprocess\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    logger.info(\"Preprocessing text columns...\")\n",
    "    \n",
    "    # Create copy to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill missing values in text columns\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('')\n",
    "    \n",
    "    # Preprocess each text column with progress bar\n",
    "    for col in text_columns:\n",
    "        logger.info(f\"Preprocessing column: {col}\")\n",
    "        processed_col = f\"{col}_processed\"\n",
    "        \n",
    "        # Process text column in batches for large datasets\n",
    "        batch_size = 10000\n",
    "        num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "        \n",
    "        processed_series = pd.Series(index=df.index, dtype='object')\n",
    "        \n",
    "        for i in tqdm(range(num_batches), desc=f\"Processing {col}\"):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(df))\n",
    "            batch = df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            processed_batch = batch[col].apply(preprocess_text)\n",
    "            processed_series.iloc[start_idx:end_idx] = processed_batch.values\n",
    "        \n",
    "        df[processed_col] = processed_series\n",
    "    \n",
    "    return df\n",
    "\n",
    "@timer_decorator\n",
    "def create_feature_combinations(df, processed_columns, weights=None):\n",
    "    \"\"\"\n",
    "    Create weighted combinations of processed text columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe with processed text columns\n",
    "    processed_columns : list\n",
    "        List of processed text columns to combine\n",
    "    weights : dict\n",
    "        Dictionary of weights for each column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        Dataframe with combined features\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating feature combinations...\")\n",
    "    \n",
    "    # Default weights\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            'job_title_processed': 3.0,\n",
    "            'descriptions_processed': 2.0,\n",
    "            'category_processed': 1.5,\n",
    "            'subcategory_processed': 1.0,\n",
    "            'role_processed': 2.0\n",
    "        }\n",
    "    \n",
    "    # Filter weights to only include columns that exist\n",
    "    weights = {col: weight for col, weight in weights.items() if col in df.columns}\n",
    "    \n",
    "    # For any processed columns not in weights, assign default weight of 1.0\n",
    "    for col in processed_columns:\n",
    "        if col not in weights:\n",
    "            weights[col] = 1.0\n",
    "    \n",
    "    # Create weighted combination\n",
    "    df['combined_features'] = ''\n",
    "    \n",
    "    for col, weight in weights.items():\n",
    "        if col in df.columns:\n",
    "            # Repeat the text according to weight\n",
    "            if weight > 1:\n",
    "                repeated_text = df[col].apply(lambda x: ' '.join([str(x)] * int(weight)))\n",
    "                df['combined_features'] += ' ' + repeated_text\n",
    "            else:\n",
    "                df['combined_features'] += ' ' + df[col].astype(str)\n",
    "    \n",
    "    # Clean up the combined features\n",
    "    df['combined_features'] = df['combined_features'].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "class JobRecommendationSystem:\n",
    "    \"\"\"\n",
    "    Job Recommendation System class that handles training, evaluation, and recommendations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.train_indices = None\n",
    "        self.test_indices = None\n",
    "        self.df = None\n",
    "        self.svd_model = None\n",
    "        self.reduced_tfidf_matrix = None\n",
    "        self.kmeans_model = None\n",
    "        self.job_clusters = None\n",
    "        \n",
    "    @timer_decorator\n",
    "    def train_recommendation_model(self, df, feature_column='combined_features', \n",
    "                                test_size=0.2, random_state=42, \n",
    "                                max_features=10000, ngram_range=(1, 2), \n",
    "                                min_df=2, max_df=0.85, use_idf=True):\n",
    "        \"\"\"\n",
    "        Train a TF-IDF based recommendation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas DataFrame\n",
    "            The data to train the model on\n",
    "        feature_column : str\n",
    "            The column to use for training\n",
    "        test_size : float\n",
    "            The proportion of the dataset to include in the test split\n",
    "        random_state : int\n",
    "            Random state for reproducibility\n",
    "        max_features : int\n",
    "            Maximum number of features for TF-IDF\n",
    "        ngram_range : tuple\n",
    "            Range of n-grams to consider\n",
    "        min_df : int or float\n",
    "            Minimum document frequency\n",
    "        max_df : float\n",
    "            Maximum document frequency\n",
    "        use_idf : bool\n",
    "            Whether to use inverse document frequency\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : JobRecommendationSystem\n",
    "            The trained model\n",
    "        \"\"\"\n",
    "        logger.info(f\"Training recommendation model with {len(df)} samples...\")\n",
    "        \n",
    "        # Store the dataframe\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        self.train_indices, self.test_indices = train_test_split(\n",
    "            range(len(df)), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training set size: {len(self.train_indices)}, Test set size: {len(self.test_indices)}\")\n",
    "        \n",
    "        # Get the training data\n",
    "        train_corpus = df.iloc[self.train_indices][feature_column].tolist()\n",
    "        \n",
    "        # Create and fit TF-IDF Vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            use_idf=use_idf,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            sublinear_tf=True  # Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Fitting TF-IDF vectorizer...\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(train_corpus)\n",
    "        logger.info(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timer_decorator\n",
    "    def apply_dimensionality_reduction(self, n_components=300):\n",
    "        \"\"\"\n",
    "        Apply dimensionality reduction to the TF-IDF matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of components for SVD\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : JobRecommendationSystem\n",
    "            The model with reduced dimensionality\n",
    "        \"\"\"\n",
    "        logger.info(f\"Applying SVD dimensionality reduction to {n_components} components...\")\n",
    "        \n",
    "        self.svd_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        self.reduced_tfidf_matrix = self.svd_model.fit_transform(self.tfidf_matrix)\n",
    "        \n",
    "        variance_ratio = self.svd_model.explained_variance_ratio_.sum()\n",
    "        logger.info(f\"Explained variance ratio: {variance_ratio:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timer_decorator\n",
    "    def cluster_jobs(self, n_clusters=15):\n",
    "        \"\"\"\n",
    "        Cluster jobs using K-means\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters : int\n",
    "            Number of clusters\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : JobRecommendationSystem\n",
    "            The model with job clusters\n",
    "        \"\"\"\n",
    "        logger.info(f\"Clustering jobs into {n_clusters} clusters...\")\n",
    "        \n",
    "        # Use the reduced matrix if available, otherwise use the original\n",
    "        matrix_to_cluster = self.reduced_tfidf_matrix if self.reduced_tfidf_matrix is not None else self.tfidf_matrix\n",
    "        \n",
    "        self.kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        self.job_clusters = self.kmeans_model.fit_predict(matrix_to_cluster)\n",
    "        \n",
    "        # Add cluster information to the dataframe\n",
    "        cluster_df = pd.DataFrame({\n",
    "            'job_idx': self.train_indices,\n",
    "            'cluster': self.job_clusters\n",
    "        })\n",
    "        \n",
    "        # Count jobs per cluster\n",
    "        cluster_counts = cluster_df['cluster'].value_counts().to_dict()\n",
    "        logger.info(f\"Jobs per cluster: {cluster_counts}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timer_decorator\n",
    "    def tune_hyperparameters(self, param_grid=None):\n",
    "        \"\"\"\n",
    "        Tune the hyperparameters of the TF-IDF vectorizer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        param_grid : dict\n",
    "            Dictionary with parameters names as keys and lists of parameter settings\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        best_params : dict\n",
    "            Best parameters\n",
    "        \"\"\"\n",
    "        # Default parameter grid\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'max_features': [5000, 10000, 15000],\n",
    "                'ngram_range': [(1, 1), (1, 2)],\n",
    "                'min_df': [2, 5],\n",
    "                'max_df': [0.8, 0.9],\n",
    "                'use_idf': [True, False]\n",
    "            }\n",
    "        \n",
    "        logger.info(\"Tuning hyperparameters...\")\n",
    "        \n",
    "        # Use a smaller sample for tuning\n",
    "        sample_size = min(5000, len(self.df))\n",
    "        sample_df = self.df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        # Prepare the data\n",
    "        X = sample_df['combined_features']\n",
    "        \n",
    "        # Create a simple pipeline\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            vectorizer,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',  # Not ideal but works for unsupervised\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Since we don't have labels, we'll use the same X as both X and y\n",
    "        # This is just to make GridSearchCV work\n",
    "        grid_search.fit(X, X)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        logger.info(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def get_recommendations(self, job_id=None, job_text=None, top_n=5, use_clusters=False):\n",
    "        \"\"\"\n",
    "        Get job recommendations based on job ID or job text\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        job_id : int or None\n",
    "            The job ID to get recommendations for\n",
    "        job_text : str or None\n",
    "            The job text to get recommendations for\n",
    "        top_n : int\n",
    "            The number of recommendations to return\n",
    "        use_clusters : bool\n",
    "            Whether to use clusters for recommendations\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        recommendations : pandas DataFrame\n",
    "            The recommended jobs\n",
    "        \"\"\"\n",
    "        if job_id is not None:\n",
    "            # Get the job index\n",
    "            try:\n",
    "                job_idx = self.df[self.df['job_id'] == job_id].index[0]\n",
    "            except IndexError:\n",
    "                logger.error(f\"Job ID {job_id} not found in the dataset\")\n",
    "                return None\n",
    "            \n",
    "            # Get the job features\n",
    "            job_features = self.df.iloc[job_idx]['combined_features']\n",
    "            \n",
    "        elif job_text is not None:\n",
    "            # Preprocess the job text\n",
    "            job_features = preprocess_text(job_text)\n",
    "            \n",
    "            # If the job text is empty after preprocessing, return None\n",
    "            if not job_features:\n",
    "                logger.error(\"Job text is empty after preprocessing\")\n",
    "                return None\n",
    "            \n",
    "            # Set job_idx to None to indicate it's a new job\n",
    "            job_idx = None\n",
    "            \n",
    "        else:\n",
    "            logger.error(\"Either job_id or job_text must be provided\")\n",
    "            return None\n",
    "        \n",
    "        # Transform the job features\n",
    "        job_vector = self.vectorizer.transform([job_features])\n",
    "        \n",
    "        # Apply dimensionality reduction if available\n",
    "        if self.svd_model is not None:\n",
    "            job_vector = self.svd_model.transform(job_vector)\n",
    "        \n",
    "        # Use clusters for recommendations if requested\n",
    "        if use_clusters and self.kmeans_model is not None:\n",
    "            # Predict the cluster of the job\n",
    "            job_cluster = self.kmeans_model.predict(job_vector)[0]\n",
    "            \n",
    "            # Get the indices of the jobs in the same cluster\n",
    "            cluster_job_indices = [idx for idx, cluster in zip(self.train_indices, self.job_clusters) \n",
    "                                if cluster == job_cluster]\n",
    "            \n",
    "            # If the job itself is in the training set, exclude it\n",
    "            if job_idx is not None and job_idx in cluster_job_indices:\n",
    "                cluster_job_indices.remove(job_idx)\n",
    "            \n",
    "            # Get the matrix for jobs in the same cluster\n",
    "            cluster_matrix = self.reduced_tfidf_matrix[[i for i, idx in enumerate(self.train_indices) \n",
    "                                                    if idx in cluster_job_indices]]\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity_scores = cosine_similarity(job_vector, cluster_matrix).flatten()\n",
    "            \n",
    "            # Get the indices of the top N similar jobs\n",
    "            top_indices = similarity_scores.argsort()[:-top_n-1:-1]\n",
    "            \n",
    "            # Map the indices back to the original dataframe indices\n",
    "            similar_job_indices = [cluster_job_indices[i] for i in top_indices]\n",
    "            \n",
    "        else:\n",
    "            # Calculate cosine similarity with all jobs in the training set\n",
    "            matrix_to_use = self.reduced_tfidf_matrix if self.svd_model is not None else self.tfidf_matrix\n",
    "            similarity_scores = cosine_similarity(job_vector, matrix_to_use).flatten()\n",
    "            \n",
    "            # Get the indices of the top N similar jobs (excluding the job itself)\n",
    "            top_indices = similarity_scores.argsort()[:-top_n-1:-1]\n",
    "            \n",
    "            # Map the indices back to the original dataframe indices\n",
    "            similar_job_indices = [self.train_indices[i] for i in top_indices \n",
    "                            if self.train_indices[i] != job_idx]\n",
    "        \n",
    "        # Get the top N similar jobs - limit to actual length of similar_job_indices\n",
    "        recommendations = self.df.iloc[similar_job_indices][\n",
    "            ['job_id', 'job_title', 'company', 'location', 'category', 'role', 'type', 'salary']\n",
    "        ].copy()\n",
    "        \n",
    "        # Create a list of the similarity scores corresponding to each job in similar_job_indices\n",
    "        # Make sure the number of similarity scores matches the number of recommendations\n",
    "        rec_scores = [similarity_scores[top_indices[i]] for i in range(len(similar_job_indices))]\n",
    "        \n",
    "        # Add similarity scores - now we're using a list of the right length\n",
    "        recommendations['similarity_score'] = rec_scores\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        recommendations = recommendations.sort_values('similarity_score', ascending=False)\n",
    "        \n",
    "        return recommendations.head(top_n)\n",
    "    \n",
    "    @timer_decorator\n",
    "    def evaluate_model(self, metric='cosine_similarity', sample_size=100):\n",
    "        \"\"\"\n",
    "        Evaluate the model using various metrics\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        metric : str\n",
    "            The metric to use for evaluation\n",
    "        sample_size : int\n",
    "            The number of test samples to evaluate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        score : float\n",
    "            The evaluation score\n",
    "        \"\"\"\n",
    "        logger.info(f\"Evaluating model using {metric} metric...\")\n",
    "        \n",
    "        # Use only sample_size jobs from the test set for evaluation\n",
    "        test_sample = min(sample_size, len(self.test_indices))\n",
    "        test_sample_indices = np.random.choice(self.test_indices, test_sample, replace=False)\n",
    "        \n",
    "        if metric == 'cosine_similarity':\n",
    "            avg_similarity = 0\n",
    "            for idx in tqdm(test_sample_indices, desc=\"Evaluating\"):\n",
    "                job_features = self.df.iloc[idx]['combined_features']\n",
    "                job_vector = self.vectorizer.transform([job_features])\n",
    "                \n",
    "                # Apply dimensionality reduction if available\n",
    "                if self.svd_model is not None:\n",
    "                    job_vector = self.svd_model.transform(job_vector)\n",
    "                    matrix_to_use = self.reduced_tfidf_matrix\n",
    "                else:\n",
    "                    matrix_to_use = self.tfidf_matrix\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity_scores = cosine_similarity(job_vector, matrix_to_use).flatten()\n",
    "                \n",
    "                # Find the 5 most similar jobs in the training set\n",
    "                top_indices = similarity_scores.argsort()[:-6:-1]\n",
    "                \n",
    "                # Calculate average similarity for this job\n",
    "                avg_job_similarity = similarity_scores[top_indices].mean()\n",
    "                avg_similarity += avg_job_similarity\n",
    "            \n",
    "            avg_similarity /= test_sample\n",
    "            return avg_similarity\n",
    "        \n",
    "        elif metric == 'category_match':\n",
    "            # Evaluate based on category/role match rate\n",
    "            match_rate = 0\n",
    "            for idx in tqdm(test_sample_indices, desc=\"Evaluating\"):\n",
    "                job_id = self.df.iloc[idx]['job_id']\n",
    "                test_job_category = self.df.iloc[idx]['category']\n",
    "                test_job_role = self.df.iloc[idx]['role']\n",
    "                \n",
    "                # Get recommendations\n",
    "                recommendations = self.get_recommendations(job_id=job_id, top_n=5)\n",
    "                \n",
    "                if recommendations is not None and not recommendations.empty:\n",
    "                    # Calculate category match rate\n",
    "                    category_matches = (recommendations['category'] == test_job_category).mean()\n",
    "                    role_matches = (recommendations['role'] == test_job_role).mean()\n",
    "                    \n",
    "                    # Weighted average\n",
    "                    job_match_rate = 0.5 * category_matches + 0.5 * role_matches\n",
    "                    match_rate += job_match_rate\n",
    "            \n",
    "            match_rate /= test_sample\n",
    "            return match_rate\n",
    "        \n",
    "        else:\n",
    "            logger.error(f\"Unknown metric: {metric}\")\n",
    "            return 0\n",
    "    \n",
    "    def save_model(self, model_dir=\"models\"):\n",
    "        \"\"\"\n",
    "        Save the model to disk\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_dir : str\n",
    "            Directory to save the model\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the vectorizer\n",
    "        with open(f\"{model_dir}/vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "        \n",
    "        # Save the TF-IDF matrix as a sparse matrix\n",
    "        save_npz(f\"{model_dir}/tfidf_matrix.npz\", self.tfidf_matrix)\n",
    "        \n",
    "        # Save the train indices\n",
    "        with open(f\"{model_dir}/train_indices.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.train_indices, f)\n",
    "        \n",
    "        # Save the SVD model if available\n",
    "        if self.svd_model is not None:\n",
    "            with open(f\"{model_dir}/svd_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.svd_model, f)\n",
    "            \n",
    "            # Save the reduced TF-IDF matrix\n",
    "            np.save(f\"{model_dir}/reduced_tfidf_matrix.npy\", self.reduced_tfidf_matrix)\n",
    "        \n",
    "        # Save the K-means model if available\n",
    "        if self.kmeans_model is not None:\n",
    "            with open(f\"{model_dir}/kmeans_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.kmeans_model, f)\n",
    "            \n",
    "            # Save the job clusters\n",
    "            np.save(f\"{model_dir}/job_clusters.npy\", self.job_clusters)\n",
    "        \n",
    "        # Save the model metadata\n",
    "        metadata = {\n",
    "            'df_columns': self.df.columns.tolist(),\n",
    "            'df_index': self.df.index.tolist(),\n",
    "            'has_svd': self.svd_model is not None,\n",
    "            'has_kmeans': self.kmeans_model is not None\n",
    "        }\n",
    "        \n",
    "        with open(f\"{model_dir}/metadata.pkl\", \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {model_dir}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, model_dir=\"models\", df=None):\n",
    "        \"\"\"\n",
    "        Load the model from disk\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_dir : str\n",
    "            Directory to load the model from\n",
    "        df : pandas DataFrame\n",
    "            The dataframe to use (if not provided, must be saved with the model)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model : JobRecommendationSystem\n",
    "            The loaded model\n",
    "        \"\"\"\n",
    "        model = cls()\n",
    "        \n",
    "        # Load the vectorizer\n",
    "        with open(f\"{model_dir}/vectorizer.pkl\", \"rb\") as f:\n",
    "            model.vectorizer = pickle.load(f)\n",
    "        \n",
    "        # Load the TF-IDF matrix\n",
    "        model.tfidf_matrix = load_npz(f\"{model_dir}/tfidf_matrix.npz\")\n",
    "        \n",
    "        # Load the train indices\n",
    "        with open(f\"{model_dir}/train_indices.pkl\", \"rb\") as f:\n",
    "            model.train_indices = pickle.load(f)\n",
    "        \n",
    "        # Load the model metadata\n",
    "        with open(f\"{model_dir}/metadata.pkl\", \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        # Load the SVD model if available\n",
    "        if metadata['has_svd'] and os.path.exists(f\"{model_dir}/svd_model.pkl\"):\n",
    "            with open(f\"{model_dir}/svd_model.pkl\", \"rb\") as f:\n",
    "                model.svd_model = pickle.load(f)\n",
    "            \n",
    "            # Load the reduced TF-IDF matrix\n",
    "            model.reduced_tfidf_matrix = np.load(f\"{model_dir}/reduced_tfidf_matrix.npy\")\n",
    "        \n",
    "        # Load the K-means model if available\n",
    "        if metadata['has_kmeans'] and os.path.exists(f\"{model_dir}/kmeans_model.pkl\"):\n",
    "            with open(f\"{model_dir}/kmeans_model.pkl\", \"rb\") as f:\n",
    "                model.kmeans_model = pickle.load(f)\n",
    "            \n",
    "            # Load the job clusters\n",
    "            model.job_clusters = np.load(f\"{model_dir}/job_clusters.npy\")\n",
    "        \n",
    "        # Set the dataframe\n",
    "        model.df = df\n",
    "        \n",
    "        logger.info(f\"Model loaded from {model_dir}\")\n",
    "        return model\n",
    "\n",
    "# Train multiple models with different configurations\n",
    "@timer_decorator\n",
    "def train_multiple_models(df, config_grid, eval_metric='cosine_similarity'):\n",
    "    \"\"\"\n",
    "    Train multiple models with different configurations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to use\n",
    "    config_grid : dict\n",
    "        Dictionary with parameter configurations\n",
    "    eval_metric : str\n",
    "        Metric to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : JobRecommendationSystem\n",
    "        The best model\n",
    "    all_models : dict\n",
    "        Dictionary with all trained models\n",
    "    results : dict\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    logger.info(\"Training multiple models with different configurations...\")\n",
    "    \n",
    "    all_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Iterate over all configurations\n",
    "    for config_name, config in config_grid.items():\n",
    "        logger.info(f\"Training model with configuration: {config_name}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        model = JobRecommendationSystem()\n",
    "        \n",
    "        # Extract parameters for training\n",
    "        train_params = {k: v for k, v in config.items() \n",
    "                      if k in ['test_size', 'random_state', 'max_features', \n",
    "                               'ngram_range', 'min_df', 'max_df', 'use_idf']}\n",
    "        \n",
    "        model.train_recommendation_model(df, **train_params)\n",
    "        \n",
    "        # Apply dimensionality reduction if specified\n",
    "        if 'n_components' in config and config['n_components'] > 0:\n",
    "            model.apply_dimensionality_reduction(n_components=config['n_components'])\n",
    "        \n",
    "        # Apply clustering if specified\n",
    "        if 'n_clusters' in config and config['n_clusters'] > 0:\n",
    "            model.cluster_jobs(n_clusters=config['n_clusters'])\n",
    "        \n",
    "        # Evaluate model\n",
    "        score = model.evaluate_model(metric=eval_metric)\n",
    "        \n",
    "        # Store model and results\n",
    "        all_models[config_name] = model\n",
    "        results[config_name] = {\n",
    "            'score': score,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Model {config_name} - {eval_metric}: {score:.4f}\")\n",
    "    \n",
    "    # Find the best model\n",
    "    best_config_name = max(results.keys(), key=lambda x: results[x]['score'])\n",
    "    best_model = all_models[best_config_name]\n",
    "    \n",
    "    logger.info(f\"Best model: {best_config_name} with {eval_metric}: {results[best_config_name]['score']:.4f}\")\n",
    "    \n",
    "    return best_model, all_models, results\n",
    "\n",
    "# Visualize model results\n",
    "def visualize_results(results, metric_name='Score', fig_size=(12, 6)):\n",
    "    \"\"\"\n",
    "    Visualize model results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Dictionary with evaluation results\n",
    "    metric_name : str\n",
    "        Name of the metric\n",
    "    fig_size : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    # Extract scores\n",
    "    models = list(results.keys())\n",
    "    scores = [results[model]['score'] for model in models]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.bar(models, scores)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Model Configuration')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'Model Performance Comparison ({metric_name})')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, score in enumerate(scores):\n",
    "        plt.text(i, score + 0.01, f'{score:.4f}', ha='center')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load the dataset\n",
    "    logger.info(\"Loading dataset...\")\n",
    "    df = pd.read_csv('jobstreet_all_job_dataset.csv')\n",
    "    logger.info(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "    \n",
    "    # Basic data exploration\n",
    "    logger.info(\"Performing basic data exploration...\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    logger.info(f\"Missing values:\\n{missing_values}\")\n",
    "    \n",
    "    # Fill missing values\n",
    "    df = df.fillna({\n",
    "        'job_title': '',\n",
    "        'descriptions': '',\n",
    "        'category': 'Unknown',\n",
    "        'subcategory': 'Unknown',\n",
    "        'role': 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Display sample of data\n",
    "    logger.info(\"Sample data:\")\n",
    "    logger.info(df.head())\n",
    "    \n",
    "    # Preprocess the dataframe\n",
    "    text_columns = ['job_title', 'descriptions', 'category', 'subcategory', 'role']\n",
    "    df = preprocess_dataframe(df, text_columns)\n",
    "    \n",
    "    # Create feature combinations with weighted columns\n",
    "    weights = {\n",
    "        'job_title_processed': 3.0,    # Job title is very important\n",
    "        'descriptions_processed': 2.0,  # Job description contains detailed requirements\n",
    "        'category_processed': 1.5,      # Category provides general field\n",
    "        'subcategory_processed': 1.0,   # Subcategory has specific domain\n",
    "        'role_processed': 2.0           # Role indicates position level\n",
    "    }\n",
    "    \n",
    "    df = create_feature_combinations(df, \n",
    "                                   [col + '_processed' for col in text_columns],\n",
    "                                   weights=weights)\n",
    "    \n",
    "    # Define model configurations to try\n",
    "    config_grid = {\n",
    "        'basic': {\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'max_features': 10000,\n",
    "            'ngram_range': (1, 1),\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.85,\n",
    "            'use_idf': True\n",
    "        },\n",
    "        'advanced': {\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'max_features': 15000,\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.8,\n",
    "            'use_idf': True,\n",
    "            'n_components': 300  # Use SVD\n",
    "        },\n",
    "        'clustered': {\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'max_features': 15000,\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.8,\n",
    "            'use_idf': True,\n",
    "            'n_components': 300,  # Use SVD\n",
    "            'n_clusters': 20      # Use clustering\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train multiple models and get the best one\n",
    "    best_model, all_models, results = train_multiple_models(df, config_grid, eval_metric='cosine_similarity')\n",
    "    \n",
    "    # Visualize the results\n",
    "    visualize_results(results, metric_name='Cosine Similarity')\n",
    "    \n",
    "    # Evaluate the best model with category match metric as well\n",
    "    category_match_score = best_model.evaluate_model(metric='category_match')\n",
    "    logger.info(f\"Best model - Category match score: {category_match_score:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    model_dir = \"job_recommender_models\"\n",
    "    best_model.save_model(model_dir)\n",
    "    \n",
    "    # Save the processed dataframe for future use\n",
    "    df[['job_id', 'job_title', 'company', 'descriptions', 'location', 'category', \n",
    "       'subcategory', 'role', 'type', 'salary', 'combined_features']].to_csv(\n",
    "        f'{model_dir}/processed_jobs.csv', index=False\n",
    "    )\n",
    "    \n",
    "    # Create a simple example recommendation\n",
    "    logger.info(\"\\nGenerating example recommendations...\")\n",
    "    \n",
    "    # Get a sample job ID\n",
    "    sample_job_id = df['job_id'].iloc[0]\n",
    "    sample_job = df[df['job_id'] == sample_job_id].iloc[0]\n",
    "    \n",
    "    logger.info(f\"\\nSample Job: {sample_job['job_title']} at {sample_job['company']}\")\n",
    "    logger.info(f\"Category: {sample_job['category']}, Role: {sample_job['role']}\")\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = best_model.get_recommendations(job_id=sample_job_id, top_n=5)\n",
    "    \n",
    "    # Display recommendations\n",
    "    logger.info(\"\\nTop 5 Recommendations:\")\n",
    "    for i, (_, rec) in enumerate(recommendations.iterrows(), 1):\n",
    "        logger.info(f\"{i}. {rec['job_title']} at {rec['company']}\")\n",
    "        logger.info(f\"   Category: {rec['category']}, Role: {rec['role']}\")\n",
    "        logger.info(f\"   Similarity Score: {rec['similarity_score']:.4f}\")\n",
    "    \n",
    "    # Get recommendations using text\n",
    "    logger.info(\"\\nGenerating recommendations from job text...\")\n",
    "    \n",
    "    # Create a sample job text\n",
    "    sample_job_text = \"\"\"\n",
    "    Data Scientist - Lead a team of data scientists to develop and implement machine learning models.\n",
    "    Requirements: 5+ years of experience in data science, Python, SQL, and machine learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get recommendations\n",
    "    text_recommendations = best_model.get_recommendations(job_text=sample_job_text, top_n=5)\n",
    "    \n",
    "    # Display recommendations\n",
    "    logger.info(\"\\nTop 5 Recommendations for sample job text:\")\n",
    "    for i, (_, rec) in enumerate(text_recommendations.iterrows(), 1):\n",
    "        logger.info(f\"{i}. {rec['job_title']} at {rec['company']}\")\n",
    "        logger.info(f\"   Category: {rec['category']}, Role: {rec['role']}\")\n",
    "        logger.info(f\"   Similarity Score: {rec['similarity_score']:.4f}\")\n",
    "    \n",
    "    # Calculate execution time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    logger.info(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    logger.info(\"\\nJob recommendation system completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
